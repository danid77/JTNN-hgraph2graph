{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 예시 입력 데이터\n",
    "a = torch.randn(2, 3).cuda()  # A matrix (2x3)\n",
    "b = torch.randn(3, 4).cuda()  # B matrix (3x4)\n",
    "\n",
    "print(\"Matrix A shape:\", a.shape)\n",
    "print(\"Matrix B shape:\", b.shape)\n",
    "\n",
    "# 매트릭스 곱셈\n",
    "c = torch.matmul(a, b)\n",
    "print(\"Matrix C shape:\", c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_Mar__8_18:18:20_PST_2022\n",
      "Cuda compilation tools, release 11.6, V11.6.124\n",
      "Build cuda_11.6.r11.6/compiler.31057947_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom C has index 0\n",
      "Atom C has index 1\n",
      "Atom O has index 2\n",
      "Atom N has index 3\n",
      "Atom C has index 4\n",
      "Atom C has index 5\n",
      "Atom C has index 6\n",
      "Atom C has index 7\n",
      "Atom N has index 8\n",
      "Atom C has index 9\n",
      "Atom C has index 10\n",
      "Atom C has index 11\n",
      "Atom C has index 12\n",
      "Atom C has index 13\n",
      "Atom C has index 14\n",
      "Atom C has index 15\n",
      "Atom C has index 16\n",
      "Atom O has index 17\n",
      "Atom N has index 18\n",
      "Atom C has index 19\n",
      "Atom C has index 20\n",
      "Atom C has index 21\n",
      "Atom C has index 22\n",
      "Atom C has index 23\n",
      "Atom C has index 24\n",
      "Atom C has index 25\n",
      "Atom C has index 26\n",
      "Atom O has index 27\n",
      "Atom C has index 28\n",
      "Atom C has index 29\n",
      "Atom C has index 30\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "# SMILES 문자열을 로드하여 분자 객체 생성\n",
    "smiles = 'CC(=O)Nc1ccc(Nc2ccc3c4c(cc(=O)n3C)-c3ccccc3C(=O)c24)cc1'\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "# 각 원자의 인덱스 출력\n",
    "for atom in mol.GetAtoms():\n",
    "    print(f'Atom {atom.GetSymbol()} has index {atom.GetIdx()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully kekulized molecule: CCO\n",
      "Successfully kekulized molecule: C1=CC=CC=C1\n",
      "Successfully kekulized molecule: CSC1=C2SC(C3=C(C(=O)[O-])N4C(=O)C(C(C)O)C4C3C)=CN2C=[N+]1C1CCNC1\n",
      "Successfully kekulized molecule: CC(=O)NC1=CC=C(NC2=CC=C3C4=C2C(=O)C2=CC=CC=C2C4=CC(=O)N3C)C=C1\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def kekulize_molecule(smiles):\n",
    "    # SMILES 문자열을 통해 분자 객체 생성\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"Error: Could not parse SMILES: {smiles}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 분자를 정화(Sanitize)하여 구조적 오류를 정리\n",
    "        Chem.SanitizeMol(mol)\n",
    "        # Kekulization 시도\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        print(f\"Successfully kekulized molecule: {Chem.MolToSmiles(mol)}\")\n",
    "    except Chem.rdchem.KekulizeException as e:\n",
    "        print(f\"Error kekulizing molecule: {smiles}, {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule: {smiles}, {e}\")\n",
    "        return None\n",
    "    \n",
    "    return mol\n",
    "\n",
    "# 테스트할 SMILES 문자열 목록\n",
    "smiles_list = [\n",
    "    \"CCO\",  # 에탄올\n",
    "    \"C1=CC=CC=C1\",  # 벤젠\n",
    "    \"CSc1c2sc(C3=C(C(=O)[O-])N4C(=O)C(C(C)O)C4C3C)cn2c[n+]1C1CCNC1\",  # 복잡한 분자\n",
    "    \"CC(=O)Nc1ccc(Nc2ccc3c4c(cc(=O)n3C)-c3ccccc3C(=O)c24)cc1\"\n",
    "]\n",
    "\n",
    "for smiles in smiles_list:\n",
    "    kekulize_molecule(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import math, random, sys\n",
    "import pickle\n",
    "import argparse\n",
    "from functools import partial\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from hgraph import MolGraph, common_atom_vocab, PairVocab\n",
    "import rdkit\n",
    "\n",
    "def to_numpy(tensors):\n",
    "    convert = lambda x : x.numpy() if type(x) is torch.Tensor else x\n",
    "    a,b,c = tensors\n",
    "    b = [convert(x) for x in b[0]], [convert(x) for x in b[1]]\n",
    "    return a, b, c\n",
    "\n",
    "def tensorize(mol_batch, vocab):\n",
    "    x = MolGraph.tensorize(mol_batch, vocab, common_atom_vocab)\n",
    "    return to_numpy(x)\n",
    "\n",
    "def tensorize_pair(mol_batch, vocab):\n",
    "    x, y = zip(*mol_batch)\n",
    "    x = MolGraph.tensorize(x, vocab, common_atom_vocab)\n",
    "    y = MolGraph.tensorize(y, vocab, common_atom_vocab)\n",
    "    return to_numpy(x)[:-1] + to_numpy(y) #no need of order for x\n",
    "\n",
    "def tensorize_cond(mol_batch, vocab):\n",
    "    x, y, cond = zip(*mol_batch)\n",
    "    cond = [map(int, c.split(',')) for c in cond]\n",
    "    cond = numpy.array(cond)\n",
    "    x = MolGraph.tensorize(x, vocab, common_atom_vocab)\n",
    "    y = MolGraph.tensorize(y, vocab, common_atom_vocab)\n",
    "    return to_numpy(x)[:-1] + to_numpy(y) + (cond,) #no need of order for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_all_data = pd.read_csv('data/chembl/all.txt')\n",
    "all_data = _all_data.iloc[:1000]\n",
    "all_data.to_csv('all_data.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KekulizeException",
     "evalue": "Can't kekulize mol.  Unkekulized atoms: 21\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/lnptest/anaconda3/envs/jtnn_test/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/lnptest/anaconda3/envs/jtnn_test/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_1817874/2161156146.py\", line 19, in tensorize\n    x = MolGraph.tensorize(mol_batch, vocab, common_atom_vocab)\n  File \"/home/lnptest/SB_jin/git_test/hgraph2graph_wandb/hgraph/mol_graph.py\", line 152, in tensorize\n    mol_batch = [MolGraph(x) for x in mol_batch]\n  File \"/home/lnptest/SB_jin/git_test/hgraph2graph_wandb/hgraph/mol_graph.py\", line 152, in <listcomp>\n    mol_batch = [MolGraph(x) for x in mol_batch]\n  File \"/home/lnptest/SB_jin/git_test/hgraph2graph_wandb/hgraph/mol_graph.py\", line 22, in __init__\n    self.order = self.label_tree()\n  File \"/home/lnptest/SB_jin/git_test/hgraph2graph_wandb/hgraph/mol_graph.py\", line 123, in label_tree\n    tree.nodes[i]['assm_cands'] = get_assm_cands(mol, hist, inter_label, pa_cls, len(inter_atoms))\n  File \"/home/lnptest/SB_jin/git_test/hgraph2graph_wandb/hgraph/chemutils.py\", line 120, in get_assm_cands\n    mol = get_clique_mol(mol, atoms)\n  File \"/home/lnptest/SB_jin/git_test/hgraph2graph_wandb/hgraph/chemutils.py\", line 111, in get_clique_mol\n    smiles = Chem.MolFragmentToSmiles(mol, atoms, kekuleSmiles=True)\nrdkit.Chem.rdchem.KekulizeException: Can't kekulize mol.  Unkekulized atoms: 21\n\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKekulizeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m batches \u001b[38;5;241m=\u001b[39m [data[i: i \u001b[38;5;241m+\u001b[39m batch_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data), batch_size)]\n\u001b[1;32m     12\u001b[0m pool \u001b[38;5;241m=\u001b[39m Pool(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensorize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     15\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda3/envs/jtnn_test/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jtnn_test/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mKekulizeException\u001b[0m: Can't kekulize mol.  Unkekulized atoms: 21\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lg = rdkit.RDLogger.logger()\n",
    "    lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "    train = 'all_data.txt'\n",
    "    vocab = 'vocab.txt'\n",
    "\n",
    "    data = [line.strip() for line in open(train)]\n",
    "    batch_size = len(data) // 16 + 1\n",
    "    batches = [data[i: i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "    pool = Pool(16)\n",
    "    all_data = pool.map(partial(tensorize, vocab=vocab), batches)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    all_data = [d for d in all_data if d is not None]\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    batches = [data[i: i + 32] for i in range(0, len(data), 32)]\n",
    "    func = partial(tensorize, vocab=vocab)\n",
    "    all_data = pool.map(func, batches)\n",
    "    all_data = [d for d in all_data if d is not None]\n",
    "    num_splits = len(all_data) // 1000\n",
    "\n",
    "    le = (len(all_data) + num_splits - 1) // num_splits\n",
    "\n",
    "    for split_id in range(num_splits):\n",
    "        st = split_id * le\n",
    "        sub_data = all_data[st: st + le]\n",
    "\n",
    "        with open(f'tensors-{split_id}.pkl', 'wb') as f:\n",
    "            pickle.dump(sub_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<rdkit.Chem.rdchem.Mol object at 0x7fa06f5d9580>\n",
      "<rdkit.Chem.rdchem.Mol object at 0x7fa06f5d9580>\n",
      "<rdkit.Chem.rdchem.Mol object at 0x7fa06f5d9580>\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def preprocess_molecule(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"Error: Could not parse SMILES: {smiles}\")\n",
    "        return None\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "    except Chem.rdchem.KekulizeException as e:\n",
    "        print(f\"Error kekulizing molecule: {smiles}, {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing molecule: {smiles}, {e}\")\n",
    "        return None\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "    return mol\n",
    "\n",
    "# 테스트할 SMILES 문자열 목록\n",
    "smiles_list = [\n",
    "    \"CCO\",  # 에탄올\n",
    "    \"C1=CC=CC=C1\",  # 벤젠\n",
    "    \"CSc1c2sc(C3=C(C(=O)[O-])N4C(=O)C(C(C)O)C4C3C)cn2c[n+]1C1CCNC1\"  # 복잡한 분자\n",
    "]\n",
    "\n",
    "for smiles in smiles_list:\n",
    "    print(preprocess_molecule(smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 환경 변수 설정\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # 사용할 GPU 번호로 설정\n",
    "\n",
    "# # CUDA 메모리 초기화\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"PyTorch CUDA Version:\", torch.version.cuda)\n",
    "# print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "# # print(\"Current CUDA Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# from torch.utils.data import DataLoader, Dataset # Dataset 추가\n",
    "# from torch.cuda.amp import GradScaler, autocast # 추가\n",
    "\n",
    "# import random\n",
    "# import pickle # 추가\n",
    "\n",
    "# import rdkit\n",
    "# import math, random, sys\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import os\n",
    "# from tqdm.auto import tqdm\n",
    "# import wandb\n",
    "# import networkx as nx\n",
    "\n",
    "# from hgraph import *\n",
    "\n",
    "# # 환경 변수 설정\n",
    "# # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # 사용할 GPU 번호로 설정\n",
    "\n",
    "# # CUDA 메모리 초기화\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# lg = rdkit.RDLogger.logger() \n",
    "# lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "# # class TensorDataset(Dataset):\n",
    "# #     def __init__(self, tensors):\n",
    "# #         self.tensors = tensors\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.tensors)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         return self.tensors[idx]\n",
    "\n",
    "# # def load_tensors(file_path):\n",
    "# #     with open(file_path, 'rb') as file:\n",
    "# #         tensors = pickle.load(file)\n",
    "# #     return tensors\n",
    "\n",
    "# # 사용자 정의 collate_fn 작성\n",
    "# # train_generator.py에서 DataLoader를 사용할 때 발생하는 TypeError는 DataLoader의 기본 collate_fn이 \n",
    "# # networkx의 DiGraph 객체를 처리할 수 없어서 발생하는 문제입니다. 이를 해결하기 위해 사용자 정의 \n",
    "# # collate_fn을 작성하여 DataLoader에 전달할 필요가 있습니다.\n",
    "# # from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "# # def custom_collate(batch):\n",
    "# #     batch = list(filter(lambda x: x is not None, batch))\n",
    "# #     if isinstance(batch[0], nx.DiGraph):\n",
    "# #         return batch\n",
    "# #     else:\n",
    "# #         return default_collate(batch)\n",
    "\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--train', required=True)\n",
    "# parser.add_argument('--vocab', required=True)\n",
    "# parser.add_argument('--atom_vocab', default=common_atom_vocab)\n",
    "# parser.add_argument('--save_dir', required=True)\n",
    "# parser.add_argument('--load_model', default=None)\n",
    "# parser.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "# parser.add_argument('--rnn_type', type=str, default='LSTM')\n",
    "# parser.add_argument('--hidden_size', type=int, default=1024) # 250\n",
    "# parser.add_argument('--embed_size', type=int, default=1024) # 250\n",
    "# parser.add_argument('--batch_size', type=int, default=10000)\n",
    "# # parser.add_argument('--batch_size', type=int, default=50)\n",
    "# parser.add_argument('--latent_size', type=int, default=32)\n",
    "# parser.add_argument('--depthT', type=int, default=15)\n",
    "# parser.add_argument('--depthG', type=int, default=15)\n",
    "# parser.add_argument('--diterT', type=int, default=1)\n",
    "# parser.add_argument('--diterG', type=int, default=3)\n",
    "# parser.add_argument('--dropout', type=float, default=0.2) # 0\n",
    "\n",
    "# parser.add_argument('--lr', type=float, default=1e-3)\n",
    "# parser.add_argument('--clip_norm', type=float, default=5.0)\n",
    "# parser.add_argument('--step_beta', type=float, default=0.001)\n",
    "# parser.add_argument('--max_beta', type=float, default=1.0)\n",
    "# parser.add_argument('--warmup', type=int, default=10000)\n",
    "# parser.add_argument('--kl_anneal_iter', type=int, default=2000)\n",
    "\n",
    "# parser.add_argument('--epoch', type=int, default=20)\n",
    "# parser.add_argument('--anneal_rate', type=float, default=0.9)\n",
    "# parser.add_argument('--anneal_iter', type=int, default=25000)\n",
    "# parser.add_argument('--print_iter', type=int, default=10000) # 배치사이즈와 동일하게 \n",
    "# parser.add_argument('--save_iter', type=int, default=100000)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "# # wandb\n",
    "# wandb.login()\n",
    "# wandb.init(project=\"jtnn_view\", entity=\"seungbeom_jin\")\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "# random.seed(args.seed)\n",
    "\n",
    "# # vocab.txt 로드 및 처리\n",
    "# vocab = [x.strip(\"\\r\\n \").split() for x in open(args.vocab)] \n",
    "# args.vocab = PairVocab(vocab)\n",
    "\n",
    "# # 학습 및 검증 데이터 로드\n",
    "# # train_tensors = load_tensors('train_processed/data/train/train_tensors.pkl')\n",
    "# # valid_tensors = load_tensors('train_processed/data/valid/valid_tensors.pkl')\n",
    "\n",
    "# # # 데이터 로더 설정\n",
    "# # train_dataset = TensorDataset(train_tensors)\n",
    "# # train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=4, \n",
    "# #                                pin_memory=True, collate_fn=custom_collate)\n",
    "# # print('train_dataset load!')\n",
    "\n",
    "# # valid_dataset = TensorDataset(valid_tensors)\n",
    "# # valid_data_loader = DataLoader(valid_dataset, batch_size=args.batch_size, num_workers=4, \n",
    "# #                                pin_memory=True, collate_fn=custom_collate)\n",
    "# # print('valid_dataset load!')\n",
    "\n",
    "# # 모델 정의\n",
    "# # model = HierVAE(args).cuda()\n",
    "# model = HierVAE(args).to(device)\n",
    "# print(\"Model #Params: %dK\" % (sum([x.nelement() for x in model.parameters()]) / 1000,))\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     if param.dim() == 1:\n",
    "#         nn.init.constant_(param, 0)\n",
    "#     else:\n",
    "#         nn.init.xavier_normal_(param)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# scaler = GradScaler()\n",
    "# scheduler = lr_scheduler.ExponentialLR(optimizer, args.anneal_rate)\n",
    "\n",
    "# if args.load_model:\n",
    "#     print('continuing from checkpoint ' + args.load_model)\n",
    "#     model_state, optimizer_state, total_step, beta = torch.load(args.load_model)\n",
    "#     model.load_state_dict(model_state)\n",
    "#     optimizer.load_state_dict(optimizer_state)\n",
    "# else:\n",
    "#     total_step = beta = 0\n",
    "\n",
    "# # 파라미터 및 그래디언트 노름 계산 함수\n",
    "# param_norm = lambda m: math.sqrt(sum([p.norm().item() ** 2 for p in m.parameters()]))\n",
    "# grad_norm = lambda m: math.sqrt(sum([p.grad.norm().item() ** 2 for p in m.parameters() if p.grad is not None]))\n",
    "\n",
    "\n",
    "# meters = torch.zeros(6).to(device)\n",
    "# total_step = 0\n",
    "# beta = 0.0\n",
    "\n",
    "# # WandB 설정\n",
    "# wandb.config.update(args)\n",
    "# wandb.watch(model, log=\"all\")\n",
    "\n",
    "# # 평가 함수\n",
    "# # def evaluate(model, data_loader):\n",
    "# #     model.eval()\n",
    "# #     meters = np.zeros(6)\n",
    "# #     with torch.no_grad():\n",
    "# #         for batch in tqdm(data_loader):\n",
    "# #             loss, kl_div, wacc, iacc, tacc, sacc = model(*batch, beta=1.0)\n",
    "# #             meters += np.array([kl_div.item(), loss.item(), wacc.item() * 100, iacc.item() * 100, tacc.item() * 100, sacc.item() * 100])\n",
    "# #     meters /= len(data_loader)\n",
    "# #     return meters\n",
    "\n",
    "# # index_scatter 함수 수정\n",
    "# def index_scatter(all_data, index, sub_data):\n",
    "#     buf = torch.zeros_like(all_data, dtype=sub_data.dtype)\n",
    "#     buf = buf.scatter_(0, index.repeat(sub_data.size(1), 1).t(), sub_data)\n",
    "#     return buf\n",
    "\n",
    "# print('training start!')\n",
    "# for epoch in range(args.epoch):\n",
    "#     # dataset = DataFolder(args.train, args.batch_size)\n",
    "#     dataset = DataFolder(args.train, args.batch_size)\n",
    "#     model.train()\n",
    "#     meters = torch.zeros(6).to(device)\n",
    "#     beta = 0.0\n",
    "\n",
    "#     # for batch in tqdm(dataset):\n",
    "#     for batch in tqdm(dataset):\n",
    "#         total_step += 1\n",
    "#         model.zero_grad()\n",
    "        \n",
    "        \n",
    "#         # PyTorch에서 Automatic Mixed Precision (AMP) 기능을 사용하는 방법입니다. \n",
    "#         # AMP는 모델 학습 시 16비트와 32비트 부동 소수점 연산을 혼합하여 사용함으로써 GPU 메모리 사용량을 줄이고 \n",
    "#         # 연산 속도를 높일 수 있습니다.\n",
    "#         with autocast():\n",
    "#             loss, kl_div, wacc, iacc, tacc, sacc = model(*batch, beta=beta)\n",
    "\n",
    "#         # 원래 코드\n",
    "#         # loss.backward()\n",
    "#         # nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "#         # optimizer.step()\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "        \n",
    "\n",
    "#         # Print kl_div to debug\n",
    "#         # print(kl_div)\n",
    "        \n",
    "#         # Ensure all relevant tensors are moved to CPU and converted to Python scalars if needed\n",
    "#         # kl_div_cpu = kl_div.cpu().item() if torch.is_tensor(kl_div) else kl_div\n",
    "#         # loss_cpu = loss.cpu().item() if torch.is_tensor(loss) else loss\n",
    "#         # wacc_cpu = wacc.cpu().item() if torch.is_tensor(wacc) else wacc * 100\n",
    "#         # iacc_cpu = iacc.cpu().item() if torch.is_tensor(iacc) else iacc * 100\n",
    "#         # tacc_cpu = tacc.cpu().item() if torch.is_tensor(tacc) else tacc * 100\n",
    "#         # sacc_cpu = sacc.cpu().item() if torch.is_tensor(sacc) else sacc * 100\n",
    "\n",
    "#         meters += torch.tensor([kl_div, loss, wacc * 100, iacc * 100, tacc * 100, sacc * 100]).to(device)\n",
    "\n",
    "#         if total_step % args.print_iter == 0:\n",
    "#             meters /= args.print_iter\n",
    "#             print(\"[%d] Beta: %.3f, KL: %.2f, loss: %.3f, Word: %.2f, %.2f, Topo: %.2f, Assm: %.2f, PNorm: %.2f, GNorm: %.2f\" % \n",
    "#                   (total_step, beta, meters[0], meters[1], meters[2], meters[3], meters[4], meters[5], param_norm(model), grad_norm(model)))\n",
    "#             sys.stdout.flush()\n",
    "            \n",
    "#             # WandB 로그 기록\n",
    "#             wandb.log({\n",
    "#                 \"batch\": total_step,\n",
    "#                 \"epoch\" : epoch,\n",
    "#                 \"beta\": beta,\n",
    "#                 \"KL Divergence\": meters[0],\n",
    "#                 \"Loss\": meters[1],\n",
    "#                 \"Word Accuracy\": meters[2],\n",
    "#                 \"Instance Accuracy\": meters[3],\n",
    "#                 \"Topology Accuracy\": meters[4],\n",
    "#                 \"Assembly Accuracy\": meters[5],\n",
    "#                 \"Parameter Norm\": param_norm(model),\n",
    "#                 \"Gradient Norm\": grad_norm(model)\n",
    "#             })\n",
    "            \n",
    "#             meters *= 0\n",
    "        \n",
    "#         if total_step % args.save_iter == 0:\n",
    "#             ckpt = (model.state_dict(), optimizer.state_dict(), total_step, beta)\n",
    "#             torch.save(ckpt, os.path.join(args.save_dir, f\"model.ckpt.{total_step}\"))\n",
    "\n",
    "#         if total_step % args.anneal_iter == 0:\n",
    "#             scheduler.step()\n",
    "#             print(\"learning rate: %.6f\" % scheduler.get_lr()[0])\n",
    "\n",
    "#         if total_step >= args.warmup and total_step % args.kl_anneal_iter == 0:\n",
    "#             beta = min(args.max_beta, beta + args.step_beta)\n",
    "    \n",
    "#     # 각 에포크가 끝날 때 검증 손실 계산\n",
    "#     # valid_metrics = evaluate(model, valid_data_loader)\n",
    "#     # print(f\"Epoch {epoch}, Train Loss: {meters[1].item()}, Validation Loss: {valid_metrics[1].item()}, KL Divergence: {valid_metrics[0].item()}, Word Accuracy: {valid_metrics[2].item()}, Instance Accuracy: {valid_metrics[3].item()}, Topology Accuracy: {valid_metrics[4].item()}, Assembly Accuracy: {valid_metrics[5].item()}\")\n",
    "    \n",
    "#     # # WandB 로그 기록\n",
    "#     # wandb.log({\n",
    "#     #     \"epoch\": epoch,\n",
    "#     #     \"train_loss\": meters[1].item(),\n",
    "#     #     \"valid_loss\": valid_metrics[1].item(),\n",
    "#     #     \"valid_kl_divergence\": valid_metrics[0].item(),\n",
    "#     #     \"valid_word_accuracy\": valid_metrics[2].item(),\n",
    "#     #     \"valid_instance_accuracy\": valid_metrics[3].item(),\n",
    "#     #     \"valid_topology_accuracy\": valid_metrics[4].item(),\n",
    "#     #     \"valid_assembly_accuracy\": valid_metrics[5].item(),\n",
    "#     #     \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "#     # })\n",
    "\n",
    "\n",
    "# # for epoch in range(args.epoch):\n",
    "# #     dataset = DataFolder(args.train, args.batch_size)\n",
    "\n",
    "# #     for batch in tqdm(dataset):\n",
    "# #         total_step += 1\n",
    "# #         model.zero_grad()\n",
    "        \n",
    "# #         loss, kl_div, wacc, iacc, tacc, sacc = model(*batch, beta=beta)\n",
    "\n",
    "# #         loss.backward()\n",
    "# #         nn.utils.clip_grad_norm_(model.parameters(), args.clip_norm)\n",
    "# #         optimizer.step()\n",
    "\n",
    "# #         # print(kl_div)\n",
    "# #         kl_div_cpu = kl_div.cpu().item() if torch.is_tensor(kl_div) else kl_div\n",
    "# #         meters = meters + np.array([kl_div, loss.item(), wacc * 100, iacc * 100, tacc * 100, sacc * 100])\n",
    "\n",
    "# #         if total_step % args.print_iter == 0:\n",
    "# #             meters /= args.print_iter\n",
    "# #             print(\"[%d] Beta: %.3f, KL: %.2f, loss: %.3f, Word: %.2f, %.2f, Topo: %.2f, Assm: %.2f, PNorm: %.2f, GNorm: %.2f\" % (total_step, beta, meters[0], meters[1], meters[2], meters[3], meters[4], meters[5], param_norm(model), grad_norm(model)))\n",
    "# #             sys.stdout.flush()\n",
    "# #             meters *= 0\n",
    "        \n",
    "# #         if total_step % args.save_iter == 0:\n",
    "# #             ckpt = (model.state_dict(), optimizer.state_dict(), total_step, beta)\n",
    "# #             torch.save(ckpt, os.path.join(args.save_dir, f\"model.ckpt.{total_step}\"))\n",
    "\n",
    "# #         if total_step % args.anneal_iter == 0:\n",
    "# #             scheduler.step()\n",
    "# #             print(\"learning rate: %.6f\" % scheduler.get_lr()[0])\n",
    "\n",
    "# #         if total_step >= args.warmup and total_step % args.kl_anneal_iter == 0:\n",
    "# #             beta = min(args.max_beta, beta + args.step_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f230bec90a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lnptest/anaconda3/envs/jtnn_test/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensors[idx]\n",
    "\n",
    "def load_tensors(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        tensors = pickle.load(file)\n",
    "    return tensors\n",
    "\n",
    "\n",
    "\n",
    "train_tensors = load_tensors('train_processed/data/train/train_tensors.pkl')\n",
    "valid_tensors = load_tensors('train_processed/data/valid/valid_tensors.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
